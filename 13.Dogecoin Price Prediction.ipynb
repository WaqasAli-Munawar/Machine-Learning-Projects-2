{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dogecoin is the reason for the recent drop in bitcoin prices. The price of Dogecoin is currently very cheap compared to bitcoin, but some financial experts, including Tesla’s CEO Elon Musk, claiming that we will see a rise in the price of Dogecoin soon. \n",
    "\n",
    "Predicting the price of a cryptocurrency is a regression problem in machine learning. Bitcoin is one of the most successful examples of cryptocurrency, but we recently saw a major drop in bitcoin prices due to dogecoin.\n",
    "\n",
    "There are many machine learning approaches that we can use for Dogecoin price prediction. We can train a machine learning model or we can also use an already available powerful model like the **Facebook Prophet Model**. But in the section below, we will be using the **autots** library in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dogecoin Price Prediction\n",
    "\n",
    "To predict future Dogecoin prices, we first need to get a dataset for this task. To get a dataset for the Dogecoin price prediction just follow the steps mentioned below:\n",
    "\n",
    "* Visit Yahoo Finance\n",
    "* Search for **Dogecoin**\n",
    "* Click on **Historical Data**\n",
    "* Click on Download**\n",
    "\n",
    "After completing the steps mentioned above we will find a dataset of historical prices of Dogecoin in our downloads folder. Now let’s get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from seaborn import regression\n",
    "sns.set()\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"DOGE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-15</td>\n",
       "      <td>0.002650</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>83794207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>117882236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>125692391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-18</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>72298803.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-19</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>91049562.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Open      High       Low     Close  Adj Close       Volume\n",
       "0  2020-10-15  0.002650  0.002651  0.002569  0.002593   0.002593   83794207.0\n",
       "1  2020-10-16  0.002595  0.002616  0.002557  0.002612   0.002612  117882236.0\n",
       "2  2020-10-17  0.002603  0.002612  0.002573  0.002581   0.002581  125692391.0\n",
       "3  2020-10-18  0.002581  0.002601  0.002577  0.002586   0.002586   72298803.0\n",
       "4  2020-10-19  0.002595  0.002619  0.002572  0.002590   0.002590   91049562.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the `close` column contains the values whose future values that we want to predict, so let’s have a closer look at the historical values of **close prices** of Dogecoin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.dropna()\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title(\"DogeCoin Price USD\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close\")\n",
    "plt.plot(data[\"Close\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be using the autots library to train a machine learning model for predicting the future prices of Dogecoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred frequency is: D\n",
      "Old data dropped by `drop_data_older_than_periods`.\n",
      "Model Number: 1 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 2 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 3 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 4 with model DatepartRegression in generation 0 of 10\n",
      "Model Number: 5 with model DatepartRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 6 with model DatepartRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 7 with model DatepartRegression in generation 0 of 10\n",
      "Template Eval Error: IndexError('tuple index out of range') in model 7: DatepartRegression\n",
      "Model Number: 8 with model ETS in generation 0 of 10\n",
      "Model Number: 9 with model ETS in generation 0 of 10\n",
      "Model Number: 10 with model GLM in generation 0 of 10\n",
      "Model Number: 11 with model GLM in generation 0 of 10\n",
      "Model Number: 12 with model GLS in generation 0 of 10\n",
      "Model Number: 13 with model GLS in generation 0 of 10\n",
      "Model Number: 14 with model GluonTS in generation 0 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 14: GluonTS\n",
      "Model Number: 15 with model GluonTS in generation 0 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 15: GluonTS\n",
      "Model Number: 16 with model GluonTS in generation 0 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 16: GluonTS\n",
      "Model Number: 17 with model GluonTS in generation 0 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 17: GluonTS\n",
      "Model Number: 18 with model GluonTS in generation 0 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 18: GluonTS\n",
      "Model Number: 19 with model GluonTS in generation 0 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 19: GluonTS\n",
      "Model Number: 20 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 21 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 22 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 23 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 24 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 25 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 26 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 27 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 28 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 29 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 30 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 31 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 31: VAR\n",
      "Model Number: 32 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 32: VAR\n",
      "Model Number: 33 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 33: VAR\n",
      "Model Number: 34 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 34: VECM\n",
      "Model Number: 35 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 35: VECM\n",
      "Model Number: 36 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 36: VECM\n",
      "Model Number: 37 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 37: VECM\n",
      "Model Number: 38 with model WindowRegression in generation 0 of 10\n",
      "Model Number: 39 with model ZeroesNaive in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 40 with model ZeroesNaive in generation 0 of 10\n",
      "Model Number: 41 with model LastValueNaive in generation 0 of 10\n",
      "Model Number: 42 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 43 with model GLS in generation 0 of 10\n",
      "Model Number: 44 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 45 with model GLM in generation 0 of 10\n",
      "Model Number: 46 with model ETS in generation 0 of 10\n",
      "Model Number: 47 with model FBProphet in generation 0 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 47: FBProphet\n",
      "Model Number: 48 with model RollingRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 49 with model GluonTS in generation 0 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 49: GluonTS\n",
      "Model Number: 50 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 51 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 51: VAR\n",
      "Model Number: 52 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 52: VECM\n",
      "Model Number: 53 with model WindowRegression in generation 0 of 10\n",
      "Template Eval Error: ValueError(\"WindowRegression regression_type='user' requires numpy >= 1.20\") in model 53: WindowRegression\n",
      "Model Number: 54 with model DatepartRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neighbors\\_regression.py:421: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n",
      "  warnings.warn(empty_warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series') in model 54: DatepartRegression\n",
      "Model Number: 55 with model UnivariateRegression in generation 0 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 55: UnivariateRegression\n",
      "Model Number: 56 with model UnivariateMotif in generation 0 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 56: UnivariateMotif\n",
      "Model Number: 57 with model NVAR in generation 0 of 10\n",
      "Model Number: 58 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 59 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 60 with model FBProphet in generation 0 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 60: FBProphet\n",
      "Model Number: 61 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 62 with model DatepartRegression in generation 0 of 10\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Model Number: 63 with model WindowRegression in generation 0 of 10\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 19s 12ms/step - loss: 1150915.7989\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1434109.2880\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 506451.2880\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 288773.4056\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 289845.4124\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 669452.6943\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 237489.6892\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 196259.2194\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 112183.5717\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 108767.2629\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 106817.7225\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 103416.9263\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 106202.9738\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 93227.7626\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 96663.7666\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 102090.4164\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 103166.4688\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 106502.4667\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 101599.0245\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 112153.2677\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 108238.6800\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 110569.1236\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86009.2524\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 95024.9827\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 101831.1131\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 97154.5499\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 96580.2996\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 105493.1573\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 109982.5411\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 114180.7330\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 104958.1546\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 99315.8893\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 93372.0377\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 100179.4854\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 94840.3178\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 98531.2751\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 103104.8380\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 5ms/step - loss: 99770.9728\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 110693.0051\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 114100.8339\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 100551.6807\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 94537.0513\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 99961.9742\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 98765.7728\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 97272.8176\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 100191.3227\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 94042.0024\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 111681.8417\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 109071.7357\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 107430.6430\n",
      "Model Number: 64 with model GLS in generation 0 of 10\n",
      "Model Number: 65 with model VECM in generation 0 of 10\n",
      "Template Eval Error: LinAlgError('SVD did not converge') in model 65: VECM\n",
      "Model Number: 66 with model GLM in generation 0 of 10\n",
      "Model Number: 67 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 67: VAR\n",
      "Model Number: 68 with model UnivariateRegression in generation 0 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 68: UnivariateRegression\n",
      "Model Number: 69 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 70 with model UnivariateRegression in generation 0 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 70: UnivariateRegression\n",
      "Model Number: 71 with model AverageValueNaive in generation 0 of 10\n",
      "Model Number: 72 with model UnivariateRegression in generation 0 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 72: UnivariateRegression\n",
      "Model Number: 73 with model GLM in generation 0 of 10\n",
      "Model Number: 74 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 74: VECM\n",
      "Model Number: 75 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 76 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 77 with model GLS in generation 0 of 10\n",
      "Model Number: 78 with model AverageValueNaive in generation 0 of 10\n",
      "Template Eval Error: ValueError('Model AverageValueNaive returned NaN for one or more series') in model 78: AverageValueNaive\n",
      "Model Number: 79 with model GluonTS in generation 0 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 79: GluonTS\n",
      "Model Number: 80 with model SeasonalNaive in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1390: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 81 with model GLS in generation 0 of 10\n",
      "Model Number: 82 with model NVAR in generation 0 of 10\n",
      "Model Number: 83 with model GLM in generation 0 of 10\n",
      "Model Number: 84 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 85 with model RollingRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2551: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 85: RollingRegression\n",
      "Model Number: 86 with model UnivariateMotif in generation 0 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 86: UnivariateMotif\n",
      "Model Number: 87 with model UnivariateRegression in generation 0 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 87: UnivariateRegression\n",
      "Model Number: 88 with model ETS in generation 0 of 10\n",
      "Model Number: 89 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 89: VAR\n",
      "Model Number: 90 with model VECM in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 90: VECM\n",
      "Model Number: 91 with model ETS in generation 0 of 10\n",
      "Model Number: 92 with model GLS in generation 0 of 10\n",
      "Model Number: 93 with model UnivariateRegression in generation 0 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 93: UnivariateRegression\n",
      "Model Number: 94 with model UnobservedComponents in generation 0 of 10\n",
      "Model Number: 95 with model UnivariateMotif in generation 0 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 95: UnivariateMotif\n",
      "Model Number: 96 with model FBProphet in generation 0 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 96: FBProphet\n",
      "Model Number: 97 with model GLM in generation 0 of 10\n",
      "Model Number: 98 with model UnivariateMotif in generation 0 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 98: UnivariateMotif\n",
      "Model Number: 99 with model GluonTS in generation 0 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 99: GluonTS\n",
      "Model Number: 100 with model WindowRegression in generation 0 of 10\n",
      "Model Number: 101 with model ZeroesNaive in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 102 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 103 with model GluonTS in generation 0 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 103: GluonTS\n",
      "Model Number: 104 with model UnivariateMotif in generation 0 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 104: UnivariateMotif\n",
      "Model Number: 105 with model SeasonalNaive in generation 0 of 10\n",
      "Model Number: 106 with model ZeroesNaive in generation 0 of 10\n",
      "Model Number: 107 with model DatepartRegression in generation 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 108 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 108: VAR\n",
      "Model Number: 109 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 109: VAR\n",
      "Model Number: 110 with model ETS in generation 0 of 10\n",
      "Model Number: 111 with model VAR in generation 0 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 111: VAR\n",
      "New Generation: 1 of 10\n",
      "Model Number: 112 with model SeasonalNaive in generation 1 of 10\n",
      "Model Number: 113 with model SeasonalNaive in generation 1 of 10\n",
      "Model Number: 114 with model SeasonalNaive in generation 1 of 10\n",
      "Model Number: 115 with model SeasonalNaive in generation 1 of 10\n",
      "Model Number: 116 with model UnobservedComponents in generation 1 of 10\n",
      "Model Number: 117 with model UnobservedComponents in generation 1 of 10\n",
      "Model Number: 118 with model UnobservedComponents in generation 1 of 10\n",
      "Model Number: 119 with model RollingRegression in generation 1 of 10\n",
      "Template Eval Error: LightGBMError('[gamma]: at least one target label is negative') in model 119: RollingRegression\n",
      "Model Number: 120 with model RollingRegression in generation 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2551: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 120: RollingRegression\n",
      "Model Number: 121 with model RollingRegression in generation 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2551: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 121: RollingRegression\n",
      "Model Number: 122 with model RollingRegression in generation 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 123 with model NVAR in generation 1 of 10\n",
      "Model Number: 124 with model NVAR in generation 1 of 10\n",
      "Model Number: 125 with model NVAR in generation 1 of 10\n",
      "Model Number: 126 with model NVAR in generation 1 of 10\n",
      "Model Number: 127 with model GLS in generation 1 of 10\n",
      "Model Number: 128 with model GLS in generation 1 of 10\n",
      "Model Number: 129 with model GLS in generation 1 of 10\n",
      "Model Number: 130 with model GLM in generation 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1226: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  endog_mu = self._clean(endog / mu)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 131 with model GLM in generation 1 of 10\n",
      "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 131: GLM\n",
      "Model Number: 132 with model GLM in generation 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1443: RuntimeWarning: invalid value encountered in log\n",
      "  endog * np.log(endog / mu) + (mu - endog))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 133 with model GLM in generation 1 of 10\n",
      "Model Number: 134 with model AverageValueNaive in generation 1 of 10\n",
      "Model Number: 135 with model AverageValueNaive in generation 1 of 10\n",
      "Template Eval Error: ValueError('zero-size array to reduction operation maximum which has no identity') in model 135: AverageValueNaive\n",
      "Model Number: 136 with model AverageValueNaive in generation 1 of 10\n",
      "Model Number: 137 with model LastValueNaive in generation 1 of 10\n",
      "Model Number: 138 with model LastValueNaive in generation 1 of 10\n",
      "Model Number: 139 with model LastValueNaive in generation 1 of 10\n",
      "Model Number: 140 with model ETS in generation 1 of 10\n",
      "Model Number: 141 with model ETS in generation 1 of 10\n",
      "Model Number: 142 with model ETS in generation 1 of 10\n",
      "Model Number: 143 with model ETS in generation 1 of 10\n",
      "Model Number: 144 with model WindowRegression in generation 1 of 10\n",
      "Model Number: 145 with model WindowRegression in generation 1 of 10\n",
      "Template Eval Error: ValueError('at least one array or dtype is required') in model 145: WindowRegression\n",
      "Model Number: 146 with model WindowRegression in generation 1 of 10\n",
      "Model Number: 147 with model ZeroesNaive in generation 1 of 10\n",
      "Model Number: 148 with model ZeroesNaive in generation 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 149 with model ZeroesNaive in generation 1 of 10\n",
      "Model Number: 150 with model DatepartRegression in generation 1 of 10\n",
      "Model Number: 151 with model DatepartRegression in generation 1 of 10\n",
      "Model Number: 152 with model DatepartRegression in generation 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 153 with model GluonTS in generation 1 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 153: GluonTS\n",
      "Model Number: 154 with model GluonTS in generation 1 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 154: GluonTS\n",
      "Model Number: 155 with model GluonTS in generation 1 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 155: GluonTS\n",
      "Model Number: 156 with model GluonTS in generation 1 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 156: GluonTS\n",
      "Model Number: 157 with model VAR in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 157: VAR\n",
      "Model Number: 158 with model VAR in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 158: VAR\n",
      "Model Number: 159 with model VAR in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 159: VAR\n",
      "Model Number: 160 with model VAR in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 160: VAR\n",
      "Model Number: 161 with model VECM in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 161: VECM\n",
      "Model Number: 162 with model VECM in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 162: VECM\n",
      "Model Number: 163 with model VECM in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 163: VECM\n",
      "Model Number: 164 with model VECM in generation 1 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 164: VECM\n",
      "Model Number: 165 with model FBProphet in generation 1 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 165: FBProphet\n",
      "Model Number: 166 with model FBProphet in generation 1 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 166: FBProphet\n",
      "Model Number: 167 with model FBProphet in generation 1 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 167: FBProphet\n",
      "Model Number: 168 with model FBProphet in generation 1 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 168: FBProphet\n",
      "Model Number: 169 with model UnivariateRegression in generation 1 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 169: UnivariateRegression\n",
      "Model Number: 170 with model UnivariateRegression in generation 1 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 170: UnivariateRegression\n",
      "Model Number: 171 with model UnivariateRegression in generation 1 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 171: UnivariateRegression\n",
      "Model Number: 172 with model UnivariateRegression in generation 1 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 172: UnivariateRegression\n",
      "Model Number: 173 with model UnivariateMotif in generation 1 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 173: UnivariateMotif\n",
      "Model Number: 174 with model UnivariateMotif in generation 1 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 174: UnivariateMotif\n",
      "Model Number: 175 with model UnivariateMotif in generation 1 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 175: UnivariateMotif\n",
      "Model Number: 176 with model UnivariateMotif in generation 1 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 176: UnivariateMotif\n",
      "New Generation: 2 of 10\n",
      "Model Number: 177 with model SeasonalNaive in generation 2 of 10\n",
      "Model Number: 178 with model SeasonalNaive in generation 2 of 10\n",
      "Model Number: 179 with model SeasonalNaive in generation 2 of 10\n",
      "Model Number: 180 with model SeasonalNaive in generation 2 of 10\n",
      "Model Number: 181 with model ETS in generation 2 of 10\n",
      "Model Number: 182 with model ETS in generation 2 of 10\n",
      "Model Number: 183 with model ETS in generation 2 of 10\n",
      "Model Number: 184 with model ETS in generation 2 of 10\n",
      "Model Number: 185 with model UnobservedComponents in generation 2 of 10\n",
      "Model Number: 186 with model UnobservedComponents in generation 2 of 10\n",
      "Model Number: 187 with model UnobservedComponents in generation 2 of 10\n",
      "Model Number: 188 with model GLM in generation 2 of 10\n",
      "Model Number: 189 with model GLM in generation 2 of 10\n",
      "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 189: GLM\n",
      "Model Number: 190 with model GLM in generation 2 of 10\n",
      "Model Number: 191 with model GLM in generation 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1230: RuntimeWarning: invalid value encountered in log\n",
      "  resid_dev -= endog_alpha * np.log(endog_alpha / mu_alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 192 with model RollingRegression in generation 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 193 with model RollingRegression in generation 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 194 with model RollingRegression in generation 2 of 10\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 194: RollingRegression\n",
      "Model Number: 195 with model RollingRegression in generation 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 196 with model NVAR in generation 2 of 10\n",
      "Model Number: 197 with model NVAR in generation 2 of 10\n",
      "Model Number: 198 with model NVAR in generation 2 of 10\n",
      "Model Number: 199 with model NVAR in generation 2 of 10\n",
      "Model Number: 200 with model GLS in generation 2 of 10\n",
      "Model Number: 201 with model GLS in generation 2 of 10\n",
      "Template Eval Error: ValueError('zero-size array to reduction operation maximum which has no identity') in model 201: GLS\n",
      "Model Number: 202 with model GLS in generation 2 of 10\n",
      "Model Number: 203 with model ZeroesNaive in generation 2 of 10\n",
      "Model Number: 204 with model ZeroesNaive in generation 2 of 10\n",
      "Template Eval Error: ValueError('NaN in DifferencedTransformer.inverse_transform') in model 204: ZeroesNaive\n",
      "Model Number: 205 with model ZeroesNaive in generation 2 of 10\n",
      "Model Number: 206 with model DatepartRegression in generation 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1390: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 207 with model DatepartRegression in generation 2 of 10\n",
      "Model Number: 208 with model DatepartRegression in generation 2 of 10\n",
      "Template Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series') in model 208: DatepartRegression\n",
      "Model Number: 209 with model AverageValueNaive in generation 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neighbors\\_regression.py:421: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n",
      "  warnings.warn(empty_warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 210 with model AverageValueNaive in generation 2 of 10\n",
      "Model Number: 211 with model AverageValueNaive in generation 2 of 10\n",
      "Model Number: 212 with model LastValueNaive in generation 2 of 10\n",
      "Model Number: 213 with model LastValueNaive in generation 2 of 10\n",
      "Model Number: 214 with model LastValueNaive in generation 2 of 10\n",
      "Model Number: 215 with model WindowRegression in generation 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 216 with model WindowRegression in generation 2 of 10\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 25ms/step - loss: 0.0586\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0595\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0523\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0420\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0394\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0371\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0353\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0352\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0325\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0331\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0310\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0346\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0287\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0298\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0305\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0300\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0296\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0283\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0292\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0292\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0268\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0287\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0280\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0272\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0257\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0280\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0261\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0269\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0281\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0238\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0257\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0259\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0267\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0272\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0257\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0248\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0246\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0254\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0231\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0253\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0237\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0240\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0263\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0234\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0250\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0242\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0229\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0251\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0245\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0237\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0236\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0246\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0234\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0234\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0220\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0239\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0228\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0246\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0231\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0241\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0216\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0238\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0240\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0221\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0210\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0231\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0230\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0215\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0223\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0218\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0238\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0241\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0219\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0231\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0210\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0229\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0237\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0225\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0211\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0223\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0226\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0208\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0218\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0222\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0227\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0214\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0202\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0225\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0219\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0205\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0199\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0207\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0223\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0212\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0212\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0195\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0213\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0214\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0201\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0204\n",
      "Model Number: 217 with model WindowRegression in generation 2 of 10\n",
      "Model Number: 218 with model GluonTS in generation 2 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 218: GluonTS\n",
      "Model Number: 219 with model GluonTS in generation 2 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 219: GluonTS\n",
      "Model Number: 220 with model GluonTS in generation 2 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 220: GluonTS\n",
      "Model Number: 221 with model GluonTS in generation 2 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 221: GluonTS\n",
      "Model Number: 222 with model VAR in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 222: VAR\n",
      "Model Number: 223 with model VAR in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 223: VAR\n",
      "Model Number: 224 with model VAR in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 224: VAR\n",
      "Model Number: 225 with model VAR in generation 2 of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 225: VAR\n",
      "Model Number: 226 with model VECM in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 226: VECM\n",
      "Model Number: 227 with model VECM in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 227: VECM\n",
      "Model Number: 228 with model VECM in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 228: VECM\n",
      "Model Number: 229 with model VECM in generation 2 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 229: VECM\n",
      "Model Number: 230 with model FBProphet in generation 2 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 230: FBProphet\n",
      "Model Number: 231 with model FBProphet in generation 2 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 231: FBProphet\n",
      "Model Number: 232 with model FBProphet in generation 2 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 232: FBProphet\n",
      "Model Number: 233 with model FBProphet in generation 2 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 233: FBProphet\n",
      "Model Number: 234 with model UnivariateRegression in generation 2 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 234: UnivariateRegression\n",
      "Model Number: 235 with model UnivariateRegression in generation 2 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 235: UnivariateRegression\n",
      "Model Number: 236 with model UnivariateRegression in generation 2 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 236: UnivariateRegression\n",
      "Model Number: 237 with model UnivariateRegression in generation 2 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 237: UnivariateRegression\n",
      "Model Number: 238 with model UnivariateMotif in generation 2 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 238: UnivariateMotif\n",
      "Model Number: 239 with model UnivariateMotif in generation 2 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 239: UnivariateMotif\n",
      "Model Number: 240 with model UnivariateMotif in generation 2 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 240: UnivariateMotif\n",
      "Model Number: 241 with model UnivariateMotif in generation 2 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 241: UnivariateMotif\n",
      "New Generation: 3 of 10\n",
      "Model Number: 242 with model SeasonalNaive in generation 3 of 10\n",
      "Model Number: 243 with model SeasonalNaive in generation 3 of 10\n",
      "Model Number: 244 with model SeasonalNaive in generation 3 of 10\n",
      "Model Number: 245 with model SeasonalNaive in generation 3 of 10\n",
      "Model Number: 246 with model RollingRegression in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 247 with model RollingRegression in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 248 with model RollingRegression in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 249 with model RollingRegression in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 250 with model ETS in generation 3 of 10\n",
      "Model Number: 251 with model ETS in generation 3 of 10\n",
      "Model Number: 252 with model ETS in generation 3 of 10\n",
      "Model Number: 253 with model ETS in generation 3 of 10\n",
      "Model Number: 254 with model AverageValueNaive in generation 3 of 10\n",
      "Model Number: 255 with model AverageValueNaive in generation 3 of 10\n",
      "Model Number: 256 with model GLM in generation 3 of 10\n",
      "Model Number: 257 with model GLM in generation 3 of 10\n",
      "Model Number: 258 with model GLM in generation 3 of 10\n",
      "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 258: GLM\n",
      "Model Number: 259 with model GLM in generation 3 of 10\n",
      "Model Number: 260 with model NVAR in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1443: RuntimeWarning: invalid value encountered in log\n",
      "  endog * np.log(endog / mu) + (mu - endog))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 261 with model NVAR in generation 3 of 10\n",
      "Model Number: 262 with model NVAR in generation 3 of 10\n",
      "Model Number: 263 with model NVAR in generation 3 of 10\n",
      "Model Number: 264 with model UnobservedComponents in generation 3 of 10\n",
      "Model Number: 265 with model UnobservedComponents in generation 3 of 10\n",
      "Model Number: 266 with model UnobservedComponents in generation 3 of 10\n",
      "Model Number: 267 with model DatepartRegression in generation 3 of 10\n",
      "Model Number: 268 with model DatepartRegression in generation 3 of 10\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 269 with model DatepartRegression in generation 3 of 10\n",
      "Model Number: 270 with model LastValueNaive in generation 3 of 10\n",
      "Model Number: 271 with model LastValueNaive in generation 3 of 10\n",
      "Model Number: 272 with model LastValueNaive in generation 3 of 10\n",
      "Model Number: 273 with model GLS in generation 3 of 10\n",
      "Model Number: 274 with model GLS in generation 3 of 10\n",
      "Model Number: 275 with model GLS in generation 3 of 10\n",
      "Model Number: 276 with model ZeroesNaive in generation 3 of 10\n",
      "Model Number: 277 with model ZeroesNaive in generation 3 of 10\n",
      "Model Number: 278 with model ZeroesNaive in generation 3 of 10\n",
      "Model Number: 279 with model WindowRegression in generation 3 of 10\n",
      "Model Number: 280 with model WindowRegression in generation 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 281 with model WindowRegression in generation 3 of 10\n",
      "Model Number: 282 with model GluonTS in generation 3 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 282: GluonTS\n",
      "Model Number: 283 with model GluonTS in generation 3 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 283: GluonTS\n",
      "Model Number: 284 with model GluonTS in generation 3 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 284: GluonTS\n",
      "Model Number: 285 with model GluonTS in generation 3 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 285: GluonTS\n",
      "Model Number: 286 with model VAR in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 286: VAR\n",
      "Model Number: 287 with model VAR in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 287: VAR\n",
      "Model Number: 288 with model VAR in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 288: VAR\n",
      "Model Number: 289 with model VECM in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 289: VECM\n",
      "Model Number: 290 with model VECM in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 290: VECM\n",
      "Model Number: 291 with model VECM in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 291: VECM\n",
      "Model Number: 292 with model VECM in generation 3 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 292: VECM\n",
      "Model Number: 293 with model FBProphet in generation 3 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 293: FBProphet\n",
      "Model Number: 294 with model FBProphet in generation 3 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 294: FBProphet\n",
      "Model Number: 295 with model FBProphet in generation 3 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 295: FBProphet\n",
      "Model Number: 296 with model FBProphet in generation 3 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 296: FBProphet\n",
      "Model Number: 297 with model UnivariateRegression in generation 3 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 297: UnivariateRegression\n",
      "Model Number: 298 with model UnivariateRegression in generation 3 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 298: UnivariateRegression\n",
      "Model Number: 299 with model UnivariateRegression in generation 3 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 299: UnivariateRegression\n",
      "Model Number: 300 with model UnivariateRegression in generation 3 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 300: UnivariateRegression\n",
      "Model Number: 301 with model UnivariateMotif in generation 3 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 301: UnivariateMotif\n",
      "Model Number: 302 with model UnivariateMotif in generation 3 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 302: UnivariateMotif\n",
      "Model Number: 303 with model UnivariateMotif in generation 3 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 303: UnivariateMotif\n",
      "Model Number: 304 with model UnivariateMotif in generation 3 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 304: UnivariateMotif\n",
      "New Generation: 4 of 10\n",
      "Model Number: 305 with model SeasonalNaive in generation 4 of 10\n",
      "Model Number: 306 with model SeasonalNaive in generation 4 of 10\n",
      "Model Number: 307 with model SeasonalNaive in generation 4 of 10\n",
      "Model Number: 308 with model SeasonalNaive in generation 4 of 10\n",
      "Model Number: 309 with model RollingRegression in generation 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 310 with model RollingRegression in generation 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 311 with model RollingRegression in generation 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 312 with model RollingRegression in generation 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 313 with model ETS in generation 4 of 10\n",
      "Model Number: 314 with model ETS in generation 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 315 with model ETS in generation 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: ValueError('Model ETS returned NaN for one or more series') in model 315: ETS\n",
      "Model Number: 316 with model ETS in generation 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: ValueError('Model ETS returned NaN for one or more series') in model 316: ETS\n",
      "Model Number: 317 with model AverageValueNaive in generation 4 of 10\n",
      "Model Number: 318 with model AverageValueNaive in generation 4 of 10\n",
      "Model Number: 319 with model AverageValueNaive in generation 4 of 10\n",
      "Model Number: 320 with model GLM in generation 4 of 10\n",
      "Template Eval Error: ValueError('X has 191 features, but QuantileTransformer is expecting 1 features as input.') in model 320: GLM\n",
      "Model Number: 321 with model GLM in generation 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1230: RuntimeWarning: invalid value encountered in log\n",
      "  resid_dev -= endog_alpha * np.log(endog_alpha / mu_alpha)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1226: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  endog_mu = self._clean(endog / mu)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 322 with model NVAR in generation 4 of 10\n",
      "Model Number: 323 with model NVAR in generation 4 of 10\n",
      "Model Number: 324 with model NVAR in generation 4 of 10\n",
      "Model Number: 325 with model NVAR in generation 4 of 10\n",
      "Model Number: 326 with model UnobservedComponents in generation 4 of 10\n",
      "Model Number: 327 with model UnobservedComponents in generation 4 of 10\n",
      "Model Number: 328 with model UnobservedComponents in generation 4 of 10\n",
      "Model Number: 329 with model LastValueNaive in generation 4 of 10\n",
      "Model Number: 330 with model LastValueNaive in generation 4 of 10\n",
      "Model Number: 331 with model LastValueNaive in generation 4 of 10\n",
      "Model Number: 332 with model DatepartRegression in generation 4 of 10\n",
      "Model Number: 333 with model DatepartRegression in generation 4 of 10\n",
      "Model Number: 334 with model DatepartRegression in generation 4 of 10\n",
      "Template Eval Error: IndexError('tuple index out of range') in model 334: DatepartRegression\n",
      "Model Number: 335 with model GLS in generation 4 of 10\n",
      "Model Number: 336 with model GLS in generation 4 of 10\n",
      "Model Number: 337 with model GLS in generation 4 of 10\n",
      "Model Number: 338 with model ZeroesNaive in generation 4 of 10\n",
      "Model Number: 339 with model ZeroesNaive in generation 4 of 10\n",
      "Model Number: 340 with model ZeroesNaive in generation 4 of 10\n",
      "Model Number: 341 with model WindowRegression in generation 4 of 10\n",
      "Model Number: 342 with model WindowRegression in generation 4 of 10\n",
      "Template Eval Error: ValueError(\"WindowRegression regression_type='user' requires numpy >= 1.20\") in model 342: WindowRegression\n",
      "Model Number: 343 with model WindowRegression in generation 4 of 10\n",
      "Template Eval Error: ValueError(\"WindowRegression regression_type='user' requires numpy >= 1.20\") in model 343: WindowRegression\n",
      "Model Number: 344 with model GluonTS in generation 4 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 344: GluonTS\n",
      "Model Number: 345 with model GluonTS in generation 4 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 345: GluonTS\n",
      "Model Number: 346 with model GluonTS in generation 4 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 346: GluonTS\n",
      "Model Number: 347 with model GluonTS in generation 4 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 347: GluonTS\n",
      "Model Number: 348 with model VAR in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 348: VAR\n",
      "Model Number: 349 with model VAR in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 349: VAR\n",
      "Model Number: 350 with model VAR in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 350: VAR\n",
      "Model Number: 351 with model VAR in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 351: VAR\n",
      "Model Number: 352 with model VECM in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 352: VECM\n",
      "Model Number: 353 with model VECM in generation 4 of 10\n",
      "Template Eval Error: AttributeError(\"'list' object has no attribute 'shape'\") in model 353: VECM\n",
      "Model Number: 354 with model VECM in generation 4 of 10\n",
      "Template Eval Error: AttributeError(\"'list' object has no attribute 'shape'\") in model 354: VECM\n",
      "Model Number: 355 with model VECM in generation 4 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 355: VECM\n",
      "Model Number: 356 with model FBProphet in generation 4 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 356: FBProphet\n",
      "Model Number: 357 with model FBProphet in generation 4 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 357: FBProphet\n",
      "Model Number: 358 with model FBProphet in generation 4 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 358: FBProphet\n",
      "Model Number: 359 with model UnivariateRegression in generation 4 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 359: UnivariateRegression\n",
      "Model Number: 360 with model UnivariateRegression in generation 4 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 360: UnivariateRegression\n",
      "Model Number: 361 with model UnivariateRegression in generation 4 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 361: UnivariateRegression\n",
      "Model Number: 362 with model UnivariateRegression in generation 4 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 362: UnivariateRegression\n",
      "Model Number: 363 with model UnivariateMotif in generation 4 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 363: UnivariateMotif\n",
      "Model Number: 364 with model UnivariateMotif in generation 4 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 364: UnivariateMotif\n",
      "Model Number: 365 with model UnivariateMotif in generation 4 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 365: UnivariateMotif\n",
      "Model Number: 366 with model UnivariateMotif in generation 4 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 366: UnivariateMotif\n",
      "New Generation: 5 of 10\n",
      "Model Number: 367 with model NVAR in generation 5 of 10\n",
      "Model Number: 368 with model NVAR in generation 5 of 10\n",
      "Model Number: 369 with model NVAR in generation 5 of 10\n",
      "Model Number: 370 with model NVAR in generation 5 of 10\n",
      "Model Number: 371 with model SeasonalNaive in generation 5 of 10\n",
      "Model Number: 372 with model SeasonalNaive in generation 5 of 10\n",
      "Model Number: 373 with model SeasonalNaive in generation 5 of 10\n",
      "Model Number: 374 with model SeasonalNaive in generation 5 of 10\n",
      "Model Number: 375 with model RollingRegression in generation 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 376 with model RollingRegression in generation 5 of 10\n",
      "Model Number: 377 with model RollingRegression in generation 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 378 with model RollingRegression in generation 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 379 with model ETS in generation 5 of 10\n",
      "Model Number: 380 with model ETS in generation 5 of 10\n",
      "Model Number: 381 with model ETS in generation 5 of 10\n",
      "Model Number: 382 with model ETS in generation 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters.py:744: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 383 with model AverageValueNaive in generation 5 of 10\n",
      "Model Number: 384 with model AverageValueNaive in generation 5 of 10\n",
      "Model Number: 385 with model AverageValueNaive in generation 5 of 10\n",
      "Model Number: 386 with model GLM in generation 5 of 10\n",
      "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 386: GLM\n",
      "Model Number: 387 with model GLM in generation 5 of 10\n",
      "Model Number: 388 with model GLM in generation 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1230: RuntimeWarning: invalid value encountered in log\n",
      "  resid_dev -= endog_alpha * np.log(endog_alpha / mu_alpha)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1226: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  endog_mu = self._clean(endog / mu)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 389 with model LastValueNaive in generation 5 of 10\n",
      "Model Number: 390 with model LastValueNaive in generation 5 of 10\n",
      "Model Number: 391 with model LastValueNaive in generation 5 of 10\n",
      "Model Number: 392 with model UnobservedComponents in generation 5 of 10\n",
      "Model Number: 393 with model UnobservedComponents in generation 5 of 10\n",
      "Model Number: 394 with model UnobservedComponents in generation 5 of 10\n",
      "Model Number: 395 with model GLS in generation 5 of 10\n",
      "Model Number: 396 with model GLS in generation 5 of 10\n",
      "Model Number: 397 with model DatepartRegression in generation 5 of 10\n",
      "Model Number: 398 with model DatepartRegression in generation 5 of 10\n",
      "Model Number: 399 with model DatepartRegression in generation 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\preprocessing\\_data.py:3237: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 400 with model ZeroesNaive in generation 5 of 10\n",
      "Model Number: 401 with model ZeroesNaive in generation 5 of 10\n",
      "Model Number: 402 with model ZeroesNaive in generation 5 of 10\n",
      "Model Number: 403 with model WindowRegression in generation 5 of 10\n",
      "Template Eval Error: ValueError(\"WindowRegression regression_type='user' requires numpy >= 1.20\") in model 403: WindowRegression\n",
      "Model Number: 404 with model WindowRegression in generation 5 of 10\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Model Number: 405 with model WindowRegression in generation 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neighbors\\_regression.py:421: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n",
      "  warnings.warn(empty_warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: ValueError('Model WindowRegression returned NaN for one or more series') in model 405: WindowRegression\n",
      "Model Number: 406 with model GluonTS in generation 5 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 406: GluonTS\n",
      "Model Number: 407 with model GluonTS in generation 5 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 407: GluonTS\n",
      "Model Number: 408 with model GluonTS in generation 5 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 408: GluonTS\n",
      "Model Number: 409 with model GluonTS in generation 5 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 409: GluonTS\n",
      "Model Number: 410 with model VAR in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 410: VAR\n",
      "Model Number: 411 with model VAR in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 411: VAR\n",
      "Model Number: 412 with model VAR in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 412: VAR\n",
      "Model Number: 413 with model VAR in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 413: VAR\n",
      "Model Number: 414 with model VECM in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 414: VECM\n",
      "Model Number: 415 with model VECM in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 415: VECM\n",
      "Model Number: 416 with model VECM in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 416: VECM\n",
      "Model Number: 417 with model VECM in generation 5 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 417: VECM\n",
      "Model Number: 418 with model FBProphet in generation 5 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 418: FBProphet\n",
      "Model Number: 419 with model FBProphet in generation 5 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 419: FBProphet\n",
      "Model Number: 420 with model FBProphet in generation 5 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 420: FBProphet\n",
      "Model Number: 421 with model FBProphet in generation 5 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 421: FBProphet\n",
      "Model Number: 422 with model UnivariateRegression in generation 5 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 422: UnivariateRegression\n",
      "Model Number: 423 with model UnivariateRegression in generation 5 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 423: UnivariateRegression\n",
      "Model Number: 424 with model UnivariateRegression in generation 5 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 424: UnivariateRegression\n",
      "Model Number: 425 with model UnivariateRegression in generation 5 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 425: UnivariateRegression\n",
      "Model Number: 426 with model UnivariateMotif in generation 5 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 426: UnivariateMotif\n",
      "Model Number: 427 with model UnivariateMotif in generation 5 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 427: UnivariateMotif\n",
      "Model Number: 428 with model UnivariateMotif in generation 5 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 428: UnivariateMotif\n",
      "Model Number: 429 with model UnivariateMotif in generation 5 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 429: UnivariateMotif\n",
      "New Generation: 6 of 10\n",
      "Model Number: 430 with model NVAR in generation 6 of 10\n",
      "Model Number: 431 with model NVAR in generation 6 of 10\n",
      "Model Number: 432 with model NVAR in generation 6 of 10\n",
      "Model Number: 433 with model NVAR in generation 6 of 10\n",
      "Model Number: 434 with model SeasonalNaive in generation 6 of 10\n",
      "Model Number: 435 with model SeasonalNaive in generation 6 of 10\n",
      "Model Number: 436 with model SeasonalNaive in generation 6 of 10\n",
      "Model Number: 437 with model SeasonalNaive in generation 6 of 10\n",
      "Model Number: 438 with model RollingRegression in generation 6 of 10\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 14ms/step - loss: 0.2287\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2266\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.2186\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.2144\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.2047\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.1960\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.1694\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.1470\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1100\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0945: 0s - loss: 0.094\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.1135\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.1063\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0962\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.1045\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0951\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.1003\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0942\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0925\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0986\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0841\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0949\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0728\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0869\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0885\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0775\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0837\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0788\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0794\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0840\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0846\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0838\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0803\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0867\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0811\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0763\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0800\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0786\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0775\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0831\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0809\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0766\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0852\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.077 - 0s 13ms/step - loss: 0.0780\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0711\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0770\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0838\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0741\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0788\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0766\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0730\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0719\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0767\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0827\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0755\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0754\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0729\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0703\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0748\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0758\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0746\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0831\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0804\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0728\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0735\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0744\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0734\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0741\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0705\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0711\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0716\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0700\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0727\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0657\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0674\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0713\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0725\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0781\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0689\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0737\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0742\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0695\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0634\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0740\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0708\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0711\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0689\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0664\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0649\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0650\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0705\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0605\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0768\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0794\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0708\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0747\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0676\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0715\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0711\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0643\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0686\n",
      "Model Number: 439 with model RollingRegression in generation 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 440 with model RollingRegression in generation 6 of 10\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 7s 6ms/step - loss: 0.0571\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0556\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0546\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0654\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0517\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0563\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0505\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0555\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0510\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0586\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0584\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0615\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0543\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0560\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0652\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0560\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0583\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0602\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0509\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0630\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0568\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0523\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0552\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0514\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0562\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0490\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0538\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0586\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0618\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0541\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0542\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0557\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0582\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0557\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0573\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0560\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0531\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0566\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0576\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0522\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0628\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0533\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0529\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0528\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0600\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0559\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0600\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0544\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0569\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0523\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0532\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0553\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0513\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0516\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0509\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0566\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0515\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0628\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0586\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0573\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0640\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0515\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0574\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0495\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0498\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0514\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0558\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0462\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0520\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0508\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0579\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0536\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0541\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0551\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0514\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0569\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0585\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0553\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0555\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0548\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0482\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0569\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0520\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0544\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0565\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0570\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0538\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0533\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0515\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0481\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0517\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0574\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0486\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0574\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0537\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0565\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0599\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0496\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0556\n",
      "Model Number: 441 with model RollingRegression in generation 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 442 with model ETS in generation 6 of 10\n",
      "Model Number: 443 with model ETS in generation 6 of 10\n",
      "Model Number: 444 with model ETS in generation 6 of 10\n",
      "Model Number: 445 with model ETS in generation 6 of 10\n",
      "Model Number: 446 with model AverageValueNaive in generation 6 of 10\n",
      "Model Number: 447 with model AverageValueNaive in generation 6 of 10\n",
      "Model Number: 448 with model AverageValueNaive in generation 6 of 10\n",
      "Model Number: 449 with model GLM in generation 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:278: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n",
      "  DomainWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 450 with model GLM in generation 6 of 10\n",
      "Model Number: 451 with model GLM in generation 6 of 10\n",
      "Model Number: 452 with model GLM in generation 6 of 10\n",
      "Model Number: 453 with model LastValueNaive in generation 6 of 10\n",
      "Model Number: 454 with model LastValueNaive in generation 6 of 10\n",
      "Model Number: 455 with model LastValueNaive in generation 6 of 10\n",
      "Model Number: 456 with model UnobservedComponents in generation 6 of 10\n",
      "Model Number: 457 with model UnobservedComponents in generation 6 of 10\n",
      "Model Number: 458 with model UnobservedComponents in generation 6 of 10\n",
      "Model Number: 459 with model GLS in generation 6 of 10\n",
      "Model Number: 460 with model GLS in generation 6 of 10\n",
      "Model Number: 461 with model GLS in generation 6 of 10\n",
      "Model Number: 462 with model DatepartRegression in generation 6 of 10\n",
      "Model Number: 463 with model DatepartRegression in generation 6 of 10\n",
      "Model Number: 464 with model DatepartRegression in generation 6 of 10\n",
      "Model Number: 465 with model ZeroesNaive in generation 6 of 10\n",
      "Model Number: 466 with model ZeroesNaive in generation 6 of 10\n",
      "Model Number: 467 with model ZeroesNaive in generation 6 of 10\n",
      "Model Number: 468 with model WindowRegression in generation 6 of 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Model Number: 469 with model WindowRegression in generation 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 470 with model WindowRegression in generation 6 of 10\n",
      "Template Eval Error: ValueError(\"WindowRegression regression_type='user' requires numpy >= 1.20\") in model 470: WindowRegression\n",
      "Model Number: 471 with model GluonTS in generation 6 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 471: GluonTS\n",
      "Model Number: 472 with model GluonTS in generation 6 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 472: GluonTS\n",
      "Model Number: 473 with model GluonTS in generation 6 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 473: GluonTS\n",
      "Model Number: 474 with model GluonTS in generation 6 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 474: GluonTS\n",
      "Model Number: 475 with model VAR in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 475: VAR\n",
      "Model Number: 476 with model VAR in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 476: VAR\n",
      "Model Number: 477 with model VAR in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 477: VAR\n",
      "Model Number: 478 with model VAR in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 478: VAR\n",
      "Model Number: 479 with model VECM in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 479: VECM\n",
      "Model Number: 480 with model VECM in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 480: VECM\n",
      "Model Number: 481 with model VECM in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 481: VECM\n",
      "Model Number: 482 with model VECM in generation 6 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 482: VECM\n",
      "Model Number: 483 with model FBProphet in generation 6 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 483: FBProphet\n",
      "Model Number: 484 with model FBProphet in generation 6 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 484: FBProphet\n",
      "Model Number: 485 with model FBProphet in generation 6 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 485: FBProphet\n",
      "Model Number: 486 with model FBProphet in generation 6 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 486: FBProphet\n",
      "Model Number: 487 with model UnivariateRegression in generation 6 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 487: UnivariateRegression\n",
      "Model Number: 488 with model UnivariateRegression in generation 6 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 488: UnivariateRegression\n",
      "Model Number: 489 with model UnivariateRegression in generation 6 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 489: UnivariateRegression\n",
      "Model Number: 490 with model UnivariateRegression in generation 6 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 490: UnivariateRegression\n",
      "Model Number: 491 with model UnivariateMotif in generation 6 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 491: UnivariateMotif\n",
      "Model Number: 492 with model UnivariateMotif in generation 6 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 492: UnivariateMotif\n",
      "Model Number: 493 with model UnivariateMotif in generation 6 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 493: UnivariateMotif\n",
      "Model Number: 494 with model UnivariateMotif in generation 6 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 494: UnivariateMotif\n",
      "New Generation: 7 of 10\n",
      "Model Number: 495 with model NVAR in generation 7 of 10\n",
      "Model Number: 496 with model NVAR in generation 7 of 10\n",
      "Model Number: 497 with model NVAR in generation 7 of 10\n",
      "Model Number: 498 with model NVAR in generation 7 of 10\n",
      "Model Number: 499 with model SeasonalNaive in generation 7 of 10\n",
      "Model Number: 500 with model SeasonalNaive in generation 7 of 10\n",
      "Model Number: 501 with model SeasonalNaive in generation 7 of 10\n",
      "Template Eval Error: ValueError('Length mismatch: Expected axis has 191 elements, new values have 1 elements') in model 501: SeasonalNaive\n",
      "Model Number: 502 with model SeasonalNaive in generation 7 of 10\n",
      "Model Number: 503 with model RollingRegression in generation 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 504 with model RollingRegression in generation 7 of 10\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 8s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0000e+0 - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0000e+00\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0000e+00\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0000e+00\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0000e+00\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0000e+00\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0000e+00\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0000e+00\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0000e+00\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0000e+00\n",
      "Model Number: 505 with model RollingRegression in generation 7 of 10\n",
      "Template Eval Error: ValueError('Found array with 0 sample(s) (shape=(0, 573)) while a minimum of 1 is required.') in model 505: RollingRegression\n",
      "Model Number: 506 with model RollingRegression in generation 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 507 with model GLS in generation 7 of 10\n",
      "Model Number: 508 with model GLS in generation 7 of 10\n",
      "Model Number: 509 with model GLS in generation 7 of 10\n",
      "Model Number: 510 with model ETS in generation 7 of 10\n",
      "Model Number: 511 with model ETS in generation 7 of 10\n",
      "Model Number: 512 with model ETS in generation 7 of 10\n",
      "Model Number: 513 with model ETS in generation 7 of 10\n",
      "Model Number: 514 with model AverageValueNaive in generation 7 of 10\n",
      "Model Number: 515 with model AverageValueNaive in generation 7 of 10\n",
      "Model Number: 516 with model AverageValueNaive in generation 7 of 10\n",
      "Model Number: 517 with model GLM in generation 7 of 10\n",
      "Template Eval Error: ValueError('Length mismatch: Expected axis has 191 elements, new values have 1 elements') in model 517: GLM\n",
      "Model Number: 518 with model GLM in generation 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1370: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input, interpolation=interpolation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 519 with model GLM in generation 7 of 10\n",
      "Model Number: 520 with model GLM in generation 7 of 10\n",
      "Template Eval Error: ValueError('Length mismatch: Expected axis has 101 elements, new values have 1 elements') in model 520: GLM\n",
      "Model Number: 521 with model LastValueNaive in generation 7 of 10\n",
      "Model Number: 522 with model LastValueNaive in generation 7 of 10\n",
      "Model Number: 523 with model LastValueNaive in generation 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1114: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1370: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input, interpolation=interpolation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 524 with model UnobservedComponents in generation 7 of 10\n",
      "Model Number: 525 with model UnobservedComponents in generation 7 of 10\n",
      "Model Number: 526 with model UnobservedComponents in generation 7 of 10\n",
      "Model Number: 527 with model DatepartRegression in generation 7 of 10\n",
      "Model Number: 528 with model DatepartRegression in generation 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 529 with model DatepartRegression in generation 7 of 10\n",
      "Template Eval Error: IndexError('tuple index out of range') in model 529: DatepartRegression\n",
      "Model Number: 530 with model ZeroesNaive in generation 7 of 10\n",
      "Model Number: 531 with model ZeroesNaive in generation 7 of 10\n",
      "Model Number: 532 with model ZeroesNaive in generation 7 of 10\n",
      "Model Number: 533 with model WindowRegression in generation 7 of 10\n",
      "Model Number: 534 with model WindowRegression in generation 7 of 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Model Number: 535 with model WindowRegression in generation 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 536 with model GluonTS in generation 7 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 536: GluonTS\n",
      "Model Number: 537 with model GluonTS in generation 7 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 537: GluonTS\n",
      "Model Number: 538 with model GluonTS in generation 7 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 538: GluonTS\n",
      "Model Number: 539 with model GluonTS in generation 7 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 539: GluonTS\n",
      "Model Number: 540 with model VAR in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 540: VAR\n",
      "Model Number: 541 with model VAR in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 541: VAR\n",
      "Model Number: 542 with model VAR in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 542: VAR\n",
      "Model Number: 543 with model VAR in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 543: VAR\n",
      "Model Number: 544 with model VECM in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 544: VECM\n",
      "Model Number: 545 with model VECM in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 545: VECM\n",
      "Model Number: 546 with model VECM in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 546: VECM\n",
      "Model Number: 547 with model VECM in generation 7 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 547: VECM\n",
      "Model Number: 548 with model FBProphet in generation 7 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 548: FBProphet\n",
      "Model Number: 549 with model FBProphet in generation 7 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 549: FBProphet\n",
      "Model Number: 550 with model FBProphet in generation 7 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 550: FBProphet\n",
      "Model Number: 551 with model FBProphet in generation 7 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 551: FBProphet\n",
      "Model Number: 552 with model UnivariateRegression in generation 7 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 552: UnivariateRegression\n",
      "Model Number: 553 with model UnivariateRegression in generation 7 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 553: UnivariateRegression\n",
      "Model Number: 554 with model UnivariateRegression in generation 7 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 554: UnivariateRegression\n",
      "Model Number: 555 with model UnivariateRegression in generation 7 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 555: UnivariateRegression\n",
      "Model Number: 556 with model UnivariateMotif in generation 7 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 556: UnivariateMotif\n",
      "Model Number: 557 with model UnivariateMotif in generation 7 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 557: UnivariateMotif\n",
      "Model Number: 558 with model UnivariateMotif in generation 7 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 558: UnivariateMotif\n",
      "Model Number: 559 with model UnivariateMotif in generation 7 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 559: UnivariateMotif\n",
      "New Generation: 8 of 10\n",
      "Model Number: 560 with model NVAR in generation 8 of 10\n",
      "Model Number: 561 with model NVAR in generation 8 of 10\n",
      "Model Number: 562 with model NVAR in generation 8 of 10\n",
      "Model Number: 563 with model NVAR in generation 8 of 10\n",
      "Model Number: 564 with model RollingRegression in generation 8 of 10\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 564: RollingRegression\n",
      "Model Number: 565 with model RollingRegression in generation 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 566 with model RollingRegression in generation 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 567 with model RollingRegression in generation 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 568 with model SeasonalNaive in generation 8 of 10\n",
      "Model Number: 569 with model SeasonalNaive in generation 8 of 10\n",
      "Model Number: 570 with model SeasonalNaive in generation 8 of 10\n",
      "Model Number: 571 with model SeasonalNaive in generation 8 of 10\n",
      "Model Number: 572 with model GLS in generation 8 of 10\n",
      "Model Number: 573 with model GLS in generation 8 of 10\n",
      "Model Number: 574 with model GLS in generation 8 of 10\n",
      "Model Number: 575 with model ETS in generation 8 of 10\n",
      "Model Number: 576 with model ETS in generation 8 of 10\n",
      "Model Number: 577 with model ETS in generation 8 of 10\n",
      "Model Number: 578 with model ETS in generation 8 of 10\n",
      "Model Number: 579 with model AverageValueNaive in generation 8 of 10\n",
      "Model Number: 580 with model AverageValueNaive in generation 8 of 10\n",
      "Model Number: 581 with model AverageValueNaive in generation 8 of 10\n",
      "Model Number: 582 with model GLM in generation 8 of 10\n",
      "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 582: GLM\n",
      "Model Number: 583 with model GLM in generation 8 of 10\n",
      "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 583: GLM\n",
      "Model Number: 584 with model GLM in generation 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1443: RuntimeWarning: invalid value encountered in log\n",
      "  endog * np.log(endog / mu) + (mu - endog))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1230: RuntimeWarning: invalid value encountered in log\n",
      "  resid_dev -= endog_alpha * np.log(endog_alpha / mu_alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 585 with model GLM in generation 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1226: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  endog_mu = self._clean(endog / mu)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 586 with model LastValueNaive in generation 8 of 10\n",
      "Model Number: 587 with model LastValueNaive in generation 8 of 10\n",
      "Model Number: 588 with model LastValueNaive in generation 8 of 10\n",
      "Model Number: 589 with model UnobservedComponents in generation 8 of 10\n",
      "Model Number: 590 with model UnobservedComponents in generation 8 of 10\n",
      "Model Number: 591 with model UnobservedComponents in generation 8 of 10\n",
      "Model Number: 592 with model DatepartRegression in generation 8 of 10\n",
      "Model Number: 593 with model DatepartRegression in generation 8 of 10\n",
      "Model Number: 594 with model DatepartRegression in generation 8 of 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 595 with model ZeroesNaive in generation 8 of 10\n",
      "Model Number: 596 with model ZeroesNaive in generation 8 of 10\n",
      "Model Number: 597 with model WindowRegression in generation 8 of 10\n",
      "Template Eval Error: ValueError(\"WindowRegression regression_type='user' requires numpy >= 1.20\") in model 597: WindowRegression\n",
      "Model Number: 598 with model WindowRegression in generation 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 599 with model WindowRegression in generation 8 of 10\n",
      "Model Number: 600 with model GluonTS in generation 8 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 600: GluonTS\n",
      "Model Number: 601 with model GluonTS in generation 8 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 601: GluonTS\n",
      "Model Number: 602 with model GluonTS in generation 8 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 602: GluonTS\n",
      "Model Number: 603 with model GluonTS in generation 8 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 603: GluonTS\n",
      "Model Number: 604 with model VAR in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 604: VAR\n",
      "Model Number: 605 with model VAR in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 605: VAR\n",
      "Model Number: 606 with model VAR in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 606: VAR\n",
      "Model Number: 607 with model VECM in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 607: VECM\n",
      "Model Number: 608 with model VECM in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 608: VECM\n",
      "Model Number: 609 with model VECM in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 609: VECM\n",
      "Model Number: 610 with model VECM in generation 8 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 610: VECM\n",
      "Model Number: 611 with model FBProphet in generation 8 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 611: FBProphet\n",
      "Model Number: 612 with model FBProphet in generation 8 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 612: FBProphet\n",
      "Model Number: 613 with model FBProphet in generation 8 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 613: FBProphet\n",
      "Model Number: 614 with model UnivariateRegression in generation 8 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 614: UnivariateRegression\n",
      "Model Number: 615 with model UnivariateRegression in generation 8 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 615: UnivariateRegression\n",
      "Model Number: 616 with model UnivariateRegression in generation 8 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 616: UnivariateRegression\n",
      "Model Number: 617 with model UnivariateRegression in generation 8 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 617: UnivariateRegression\n",
      "Model Number: 618 with model UnivariateMotif in generation 8 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 618: UnivariateMotif\n",
      "Model Number: 619 with model UnivariateMotif in generation 8 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 619: UnivariateMotif\n",
      "Model Number: 620 with model UnivariateMotif in generation 8 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 620: UnivariateMotif\n",
      "Model Number: 621 with model UnivariateMotif in generation 8 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 621: UnivariateMotif\n",
      "New Generation: 9 of 10\n",
      "Model Number: 622 with model NVAR in generation 9 of 10\n",
      "Model Number: 623 with model NVAR in generation 9 of 10\n",
      "Model Number: 624 with model NVAR in generation 9 of 10\n",
      "Model Number: 625 with model NVAR in generation 9 of 10\n",
      "Model Number: 626 with model RollingRegression in generation 9 of 10\n",
      "Model Number: 627 with model RollingRegression in generation 9 of 10\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\") in model 627: RollingRegression\n",
      "Model Number: 628 with model RollingRegression in generation 9 of 10\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 628: RollingRegression\n",
      "Model Number: 629 with model RollingRegression in generation 9 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 630 with model SeasonalNaive in generation 9 of 10\n",
      "Model Number: 631 with model SeasonalNaive in generation 9 of 10\n",
      "Model Number: 632 with model SeasonalNaive in generation 9 of 10\n",
      "Model Number: 633 with model SeasonalNaive in generation 9 of 10\n",
      "Model Number: 634 with model GLS in generation 9 of 10\n",
      "Model Number: 635 with model GLS in generation 9 of 10\n",
      "Model Number: 636 with model GLS in generation 9 of 10\n",
      "Model Number: 637 with model AverageValueNaive in generation 9 of 10\n",
      "Model Number: 638 with model AverageValueNaive in generation 9 of 10\n",
      "Model Number: 639 with model AverageValueNaive in generation 9 of 10\n",
      "Model Number: 640 with model ETS in generation 9 of 10\n",
      "Model Number: 641 with model ETS in generation 9 of 10\n",
      "Model Number: 642 with model ETS in generation 9 of 10\n",
      "Model Number: 643 with model ETS in generation 9 of 10\n",
      "Model Number: 644 with model GLM in generation 9 of 10\n",
      "Model Number: 645 with model GLM in generation 9 of 10\n",
      "Model Number: 646 with model GLM in generation 9 of 10\n",
      "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 646: GLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1226: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  endog_mu = self._clean(endog / mu)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1230: RuntimeWarning: divide by zero encountered in log\n",
      "  resid_dev -= endog_alpha * np.log(endog_alpha / mu_alpha)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1230: RuntimeWarning: invalid value encountered in multiply\n",
      "  resid_dev -= endog_alpha * np.log(endog_alpha / mu_alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 647 with model GLM in generation 9 of 10\n",
      "Model Number: 648 with model LastValueNaive in generation 9 of 10\n",
      "Model Number: 649 with model LastValueNaive in generation 9 of 10\n",
      "Model Number: 650 with model LastValueNaive in generation 9 of 10\n",
      "Model Number: 651 with model ZeroesNaive in generation 9 of 10\n",
      "Model Number: 652 with model ZeroesNaive in generation 9 of 10\n",
      "Model Number: 653 with model ZeroesNaive in generation 9 of 10\n",
      "Model Number: 654 with model UnobservedComponents in generation 9 of 10\n",
      "Model Number: 655 with model UnobservedComponents in generation 9 of 10\n",
      "Model Number: 656 with model UnobservedComponents in generation 9 of 10\n",
      "Model Number: 657 with model DatepartRegression in generation 9 of 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Model Number: 658 with model DatepartRegression in generation 9 of 10\n",
      "Model Number: 659 with model DatepartRegression in generation 9 of 10\n",
      "Model Number: 660 with model WindowRegression in generation 9 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 661 with model WindowRegression in generation 9 of 10\n",
      "Model Number: 662 with model WindowRegression in generation 9 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:1135: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 663 with model GluonTS in generation 9 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 663: GluonTS\n",
      "Model Number: 664 with model GluonTS in generation 9 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 664: GluonTS\n",
      "Model Number: 665 with model GluonTS in generation 9 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 665: GluonTS\n",
      "Model Number: 666 with model GluonTS in generation 9 of 10\n",
      "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 666: GluonTS\n",
      "Model Number: 667 with model VAR in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 667: VAR\n",
      "Model Number: 668 with model VAR in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 668: VAR\n",
      "Model Number: 669 with model VAR in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 669: VAR\n",
      "Model Number: 670 with model VAR in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VAR') in model 670: VAR\n",
      "Model Number: 671 with model VECM in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 671: VECM\n",
      "Model Number: 672 with model VECM in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 672: VECM\n",
      "Model Number: 673 with model VECM in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 673: VECM\n",
      "Model Number: 674 with model VECM in generation 9 of 10\n",
      "Template Eval Error: ValueError('Only gave one variable to VECM') in model 674: VECM\n",
      "Model Number: 675 with model FBProphet in generation 9 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 675: FBProphet\n",
      "Model Number: 676 with model FBProphet in generation 9 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 676: FBProphet\n",
      "Model Number: 677 with model FBProphet in generation 9 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 677: FBProphet\n",
      "Model Number: 678 with model FBProphet in generation 9 of 10\n",
      "Template Eval Error: ImportError('Package prophet is required') in model 678: FBProphet\n",
      "Model Number: 679 with model UnivariateRegression in generation 9 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 679: UnivariateRegression\n",
      "Model Number: 680 with model UnivariateRegression in generation 9 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 680: UnivariateRegression\n",
      "Model Number: 681 with model UnivariateRegression in generation 9 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 681: UnivariateRegression\n",
      "Model Number: 682 with model UnivariateRegression in generation 9 of 10\n",
      "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 682: UnivariateRegression\n",
      "Model Number: 683 with model UnivariateMotif in generation 9 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 683: UnivariateMotif\n",
      "Model Number: 684 with model UnivariateMotif in generation 9 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 684: UnivariateMotif\n",
      "Model Number: 685 with model UnivariateMotif in generation 9 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 685: UnivariateMotif\n",
      "Model Number: 686 with model UnivariateMotif in generation 9 of 10\n",
      "Template Eval Error: AttributeError(\"module 'numpy.lib.stride_tricks' has no attribute 'sliding_window_view'\") in model 686: UnivariateMotif\n",
      "New Generation: 10 of 10\n",
      "Model Number: 687 with model NVAR in generation 10 of 10\n",
      "Model Number: 688 with model NVAR in generation 10 of 10\n",
      "Model Number: 689 with model NVAR in generation 10 of 10\n",
      "Model Number: 690 with model NVAR in generation 10 of 10\n",
      "Model Number: 691 with model RollingRegression in generation 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 692 with model RollingRegression in generation 10 of 10\n",
      "Model Number: 693 with model RollingRegression in generation 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 694 with model RollingRegression in generation 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 695 with model SeasonalNaive in generation 10 of 10\n",
      "Model Number: 696 with model SeasonalNaive in generation 10 of 10\n",
      "Model Number: 697 with model SeasonalNaive in generation 10 of 10\n",
      "Model Number: 698 with model SeasonalNaive in generation 10 of 10\n",
      "Model Number: 699 with model AverageValueNaive in generation 10 of 10\n",
      "Model Number: 700 with model AverageValueNaive in generation 10 of 10\n",
      "Model Number: 701 with model AverageValueNaive in generation 10 of 10\n",
      "Model Number: 702 with model GLS in generation 10 of 10\n",
      "Model Number: 703 with model GLS in generation 10 of 10\n",
      "Model Number: 704 with model GLS in generation 10 of 10\n",
      "Model Number: 705 with model ETS in generation 10 of 10\n",
      "Model Number: 706 with model ETS in generation 10 of 10\n",
      "Model Number: 707 with model ETS in generation 10 of 10\n",
      "Model Number: 708 with model GLM in generation 10 of 10\n",
      "Model Number: 709 with model GLM in generation 10 of 10\n",
      "Model Number: 710 with model GLM in generation 10 of 10\n",
      "Model Number: 711 with model GLM in generation 10 of 10\n",
      "Model Number: 712 with model LastValueNaive in generation 10 of 10\n",
      "Model Number: 713 with model LastValueNaive in generation 10 of 10\n",
      "Model Number: 714 with model LastValueNaive in generation 10 of 10\n",
      "Model Number: 715 with model ZeroesNaive in generation 10 of 10\n",
      "Model Number: 716 with model ZeroesNaive in generation 10 of 10\n",
      "Model Number: 717 with model UnobservedComponents in generation 10 of 10\n",
      "Model Number: 718 with model UnobservedComponents in generation 10 of 10\n",
      "Model Number: 719 with model UnobservedComponents in generation 10 of 10\n",
      "Model Number: 720 with model DatepartRegression in generation 10 of 10\n",
      "Template Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series') in model 720: DatepartRegression\n",
      "Model Number: 721 with model DatepartRegression in generation 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neighbors\\_regression.py:421: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n",
      "  warnings.warn(empty_warning_msg)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 722 with model DatepartRegression in generation 10 of 10\n",
      "Model Number: 723 with model Ensemble in generation 11 of 0\n",
      "Model Number: 724 with model Ensemble in generation 11 of 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\preprocessing\\_data.py:3237: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 725 with model Ensemble in generation 11 of 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Round: 1\n",
      "Model Number: 1 of 99 with model NVAR for Validation 1\n",
      "Model 1 of model NVAR with avg smape 11.17: \n",
      "Model Number: 2 of 99 with model NVAR for Validation 1\n",
      "Model 2 of model NVAR with avg smape 11.17: \n",
      "Model Number: 3 of 99 with model Ensemble for Validation 1\n",
      "Model 3 of model Ensemble with avg smape 11.21: \n",
      "Model Number: 4 of 99 with model NVAR for Validation 1\n",
      "Model 4 of model NVAR with avg smape 11.29: \n",
      "Model Number: 5 of 99 with model NVAR for Validation 1\n",
      "Model 5 of model NVAR with avg smape 11.29: \n",
      "Model Number: 6 of 99 with model Ensemble for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6 of model Ensemble with avg smape 8.53: \n",
      "Model Number: 7 of 99 with model NVAR for Validation 1\n",
      "Model 7 of model NVAR with avg smape 8.75: \n",
      "Model Number: 8 of 99 with model RollingRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 8 of model RollingRegression with avg smape 7.32: \n",
      "Model Number: 9 of 99 with model SeasonalNaive for Validation 1\n",
      "Model 9 of model SeasonalNaive with avg smape 7.56: \n",
      "Model Number: 10 of 99 with model SeasonalNaive for Validation 1\n",
      "Model 10 of model SeasonalNaive with avg smape 7.44: \n",
      "Model Number: 11 of 99 with model RollingRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 11 of model RollingRegression with avg smape 7.18: \n",
      "Model Number: 12 of 99 with model AverageValueNaive for Validation 1\n",
      "Model 12 of model AverageValueNaive with avg smape 14.26: \n",
      "Model Number: 13 of 99 with model SeasonalNaive for Validation 1\n",
      "Model 13 of model SeasonalNaive with avg smape 7.63: \n",
      "Model Number: 14 of 99 with model Ensemble for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\preprocessing\\_data.py:3237: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 14 of model Ensemble with avg smape 0.0: \n",
      "Model Number: 15 of 99 with model NVAR for Validation 1\n",
      "Model 15 of model NVAR with avg smape 5.98: \n",
      "Model Number: 16 of 99 with model SeasonalNaive for Validation 1\n",
      "Model 16 of model SeasonalNaive with avg smape 5.71: \n",
      "Model Number: 17 of 99 with model NVAR for Validation 1\n",
      "Model 17 of model NVAR with avg smape 5.69: \n",
      "Model Number: 18 of 99 with model GLS for Validation 1\n",
      "Model 18 of model GLS with avg smape 13.69: \n",
      "Model Number: 19 of 99 with model AverageValueNaive for Validation 1\n",
      "Model 19 of model AverageValueNaive with avg smape 7.71: \n",
      "Model Number: 20 of 99 with model ETS for Validation 1\n",
      "Model 20 of model ETS with avg smape 7.19: \n",
      "Model Number: 21 of 99 with model GLS for Validation 1\n",
      "Model 21 of model GLS with avg smape 11.94: \n",
      "Model Number: 22 of 99 with model GLS for Validation 1\n",
      "Model 22 of model GLS with avg smape 11.94: \n",
      "Model Number: 23 of 99 with model SeasonalNaive for Validation 1\n",
      "Model 23 of model SeasonalNaive with avg smape 4.69: \n",
      "Model Number: 24 of 99 with model SeasonalNaive for Validation 1\n",
      "Model 24 of model SeasonalNaive with avg smape 7.33: \n",
      "Model Number: 25 of 99 with model GLM for Validation 1\n",
      "Model 25 of model GLM with avg smape 16.76: \n",
      "Model Number: 26 of 99 with model GLM for Validation 1\n",
      "Model 26 of model GLM with avg smape 15.61: \n",
      "Model Number: 27 of 99 with model GLM for Validation 1\n",
      "Model 27 of model GLM with avg smape 15.61: \n",
      "Model Number: 28 of 99 with model GLM for Validation 1\n",
      "Model 28 of model GLM with avg smape 15.98: \n",
      "Model Number: 29 of 99 with model GLM for Validation 1\n",
      "Model 29 of model GLM with avg smape 16.02: \n",
      "Model Number: 30 of 99 with model GLM for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:278: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n",
      "  DomainWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 30 of model GLM with avg smape 16.13: \n",
      "Model Number: 31 of 99 with model GLM for Validation 1\n",
      "Model 31 of model GLM with avg smape 16.01: \n",
      "Model Number: 32 of 99 with model GLM for Validation 1\n",
      "Model 32 of model GLM with avg smape 16.01: \n",
      "Model Number: 33 of 99 with model LastValueNaive for Validation 1\n",
      "Model 33 of model LastValueNaive with avg smape 6.95: \n",
      "Model Number: 34 of 99 with model GLS for Validation 1\n",
      "Model 34 of model GLS with avg smape 12.96: \n",
      "Model Number: 35 of 99 with model AverageValueNaive for Validation 1\n",
      "Model 35 of model AverageValueNaive with avg smape 12.99: \n",
      "Model Number: 36 of 99 with model LastValueNaive for Validation 1\n",
      "Model 36 of model LastValueNaive with avg smape 6.95: \n",
      "Model Number: 37 of 99 with model LastValueNaive for Validation 1\n",
      "Model 37 of model LastValueNaive with avg smape 6.95: \n",
      "Model Number: 38 of 99 with model LastValueNaive for Validation 1\n",
      "Model 38 of model LastValueNaive with avg smape 6.95: \n",
      "Model Number: 39 of 99 with model LastValueNaive for Validation 1\n",
      "Model 39 of model LastValueNaive with avg smape 6.95: \n",
      "Model Number: 40 of 99 with model LastValueNaive for Validation 1\n",
      "Model 40 of model LastValueNaive with avg smape 6.95: \n",
      "Model Number: 41 of 99 with model ZeroesNaive for Validation 1\n",
      "Model 41 of model ZeroesNaive with avg smape 17.3: \n",
      "Model Number: 42 of 99 with model SeasonalNaive for Validation 1\n",
      "Model 42 of model SeasonalNaive with avg smape 10.33: \n",
      "Model Number: 43 of 99 with model NVAR for Validation 1\n",
      "Model 43 of model NVAR with avg smape 6.94: \n",
      "Model Number: 44 of 99 with model SeasonalNaive for Validation 1\n",
      "Model 44 of model SeasonalNaive with avg smape 8.13: \n",
      "Model Number: 45 of 99 with model UnobservedComponents for Validation 1\n",
      "Model 45 of model UnobservedComponents with avg smape 7.19: \n",
      "Model Number: 46 of 99 with model UnobservedComponents for Validation 1\n",
      "Model 46 of model UnobservedComponents with avg smape 7.19: \n",
      "Model Number: 47 of 99 with model UnobservedComponents for Validation 1\n",
      "Model 47 of model UnobservedComponents with avg smape 7.19: \n",
      "Model Number: 48 of 99 with model UnobservedComponents for Validation 1\n",
      "Model 48 of model UnobservedComponents with avg smape 7.19: \n",
      "Model Number: 49 of 99 with model GLS for Validation 1\n",
      "Model 49 of model GLS with avg smape 10.98: \n",
      "Model Number: 50 of 99 with model GLS for Validation 1\n",
      "Model 50 of model GLS with avg smape 10.98: \n",
      "Model Number: 51 of 99 with model AverageValueNaive for Validation 1\n",
      "Model 51 of model AverageValueNaive with avg smape 17.34: \n",
      "Model Number: 52 of 99 with model LastValueNaive for Validation 1\n",
      "Model 52 of model LastValueNaive with avg smape 17.04: \n",
      "Model Number: 53 of 99 with model RollingRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 53 of model RollingRegression with avg smape 7.36: \n",
      "Model Number: 54 of 99 with model UnobservedComponents for Validation 1\n",
      "Model 54 of model UnobservedComponents with avg smape 6.95: \n",
      "Model Number: 55 of 99 with model RollingRegression for Validation 1\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 14ms/step - loss: 0.2215\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.2319\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2341\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.2106\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.2186\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.1957\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1775\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.1478\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1124\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.1207\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1176\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1097\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0961\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0972\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.1082\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0972\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0861\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.1037\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0965\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0835\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0898\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0814\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0824\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0779\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0830\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0918\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0757\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0838\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0790\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0774\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0889\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0964\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0851\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0750\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0862\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0783\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0819\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0885\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0901\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0848\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0785\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0785\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0847\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0776\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0805\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0777\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0854\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0820\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0867\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0747\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0834\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0760\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0762\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0784\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0737\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0794\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0821\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0812\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0753\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0712\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0758\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0778\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0732\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0835\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0778\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0779\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0782\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0803\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0784\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0712\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0725\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0708\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0784\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0785\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0756\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0729\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0669\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0773\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0755\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0722\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0731\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0823\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0698\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0688\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0720\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0675\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0673\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0759\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0808\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0676\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0752\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0752\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0736\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0715\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0693\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0717\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0733\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0766\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0682\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0724\n",
      "Model 55 of model RollingRegression with avg smape 9.72: \n",
      "Model Number: 56 of 99 with model UnobservedComponents for Validation 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 56 of model UnobservedComponents with avg smape 6.95: \n",
      "Model Number: 57 of 99 with model UnobservedComponents for Validation 1\n",
      "Model 57 of model UnobservedComponents with avg smape 8.25: \n",
      "Model Number: 58 of 99 with model DatepartRegression for Validation 1\n",
      "Model 58 of model DatepartRegression with avg smape 12.35: \n",
      "Model Number: 59 of 99 with model ETS for Validation 1\n",
      "Model 59 of model ETS with avg smape 7.19: \n",
      "Model Number: 60 of 99 with model DatepartRegression for Validation 1\n",
      "Model 60 of model DatepartRegression with avg smape 6.63: \n",
      "Model Number: 61 of 99 with model DatepartRegression for Validation 1\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 61 of model DatepartRegression with avg smape 14.39: \n",
      "Model Number: 62 of 99 with model RollingRegression for Validation 1\n",
      "Model 62 of model RollingRegression with avg smape 6.95: \n",
      "Model Number: 63 of 99 with model GLS for Validation 1\n",
      "Model 63 of model GLS with avg smape 12.77: \n",
      "Model Number: 64 of 99 with model GLS for Validation 1\n",
      "Model 64 of model GLS with avg smape 12.77: \n",
      "Model Number: 65 of 99 with model ZeroesNaive for Validation 1\n",
      "Model 65 of model ZeroesNaive with avg smape 6.95: \n",
      "Model Number: 66 of 99 with model ZeroesNaive for Validation 1\n",
      "Model 66 of model ZeroesNaive with avg smape 6.95: \n",
      "Model Number: 67 of 99 with model AverageValueNaive for Validation 1\n",
      "Model 67 of model AverageValueNaive with avg smape 10.19: \n",
      "Model Number: 68 of 99 with model ZeroesNaive for Validation 1\n",
      "Model 68 of model ZeroesNaive with avg smape 6.93: \n",
      "Model Number: 69 of 99 with model RollingRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 69 of model RollingRegression with avg smape 7.21: \n",
      "Model Number: 70 of 99 with model ETS for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 70 of model ETS with avg smape 17.3: \n",
      "Model Number: 71 of 99 with model ETS for Validation 1\n",
      "Model 71 of model ETS with avg smape 15.0: \n",
      "Model Number: 72 of 99 with model LastValueNaive for Validation 1\n",
      "Model 72 of model LastValueNaive with avg smape 6.95: \n",
      "Model Number: 73 of 99 with model ETS for Validation 1\n",
      "Model 73 of model ETS with avg smape 5.91: \n",
      "Model Number: 74 of 99 with model RollingRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 74 of model RollingRegression with avg smape 7.5: \n",
      "Model Number: 75 of 99 with model UnobservedComponents for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 75 of model UnobservedComponents with avg smape 16.8: \n",
      "Model Number: 76 of 99 with model DatepartRegression for Validation 1\n",
      "Model 76 of model DatepartRegression with avg smape 7.19: \n",
      "Model Number: 77 of 99 with model DatepartRegression for Validation 1\n",
      "Model 77 of model DatepartRegression with avg smape 15.48: \n",
      "Model Number: 78 of 99 with model DatepartRegression for Validation 1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Model 78 of model DatepartRegression with avg smape 10.52: \n",
      "Model Number: 79 of 99 with model RollingRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 79 of model RollingRegression with avg smape 10.12: \n",
      "Model Number: 80 of 99 with model DatepartRegression for Validation 1\n",
      "Model 80 of model DatepartRegression with avg smape 13.22: \n",
      "Model Number: 81 of 99 with model ETS for Validation 1\n",
      "Model 81 of model ETS with avg smape 7.19: \n",
      "Model Number: 82 of 99 with model AverageValueNaive for Validation 1\n",
      "Model 82 of model AverageValueNaive with avg smape 6.46: \n",
      "Model Number: 83 of 99 with model DatepartRegression for Validation 1\n",
      "Model 83 of model DatepartRegression with avg smape 6.79: \n",
      "Model Number: 84 of 99 with model ZeroesNaive for Validation 1\n",
      "Model 84 of model ZeroesNaive with avg smape 6.93: \n",
      "Model Number: 85 of 99 with model ZeroesNaive for Validation 1\n",
      "Model 85 of model ZeroesNaive with avg smape 6.93: \n",
      "Model Number: 86 of 99 with model AverageValueNaive for Validation 1\n",
      "Model 86 of model AverageValueNaive with avg smape 14.31: \n",
      "Model Number: 87 of 99 with model ZeroesNaive for Validation 1\n",
      "Model 87 of model ZeroesNaive with avg smape 6.95: \n",
      "Model Number: 88 of 99 with model ZeroesNaive for Validation 1\n",
      "Model 88 of model ZeroesNaive with avg smape 6.95: \n",
      "Model Number: 89 of 99 with model ETS for Validation 1\n",
      "Model 89 of model ETS with avg smape 6.95: \n",
      "Model Number: 90 of 99 with model AverageValueNaive for Validation 1\n",
      "Model 90 of model AverageValueNaive with avg smape 6.95: \n",
      "Model Number: 91 of 99 with model ETS for Validation 1\n",
      "Model 91 of model ETS with avg smape 6.95: \n",
      "Model Number: 92 of 99 with model WindowRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 92 of model WindowRegression with avg smape 9.01: \n",
      "Model Number: 93 of 99 with model WindowRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 93 of model WindowRegression with avg smape 10.16: \n",
      "Model Number: 94 of 99 with model WindowRegression for Validation 1\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 7s 6ms/step - loss: 0.0651\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0616\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0538\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0480\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0461\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0371\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0396\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0368\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0340\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0339\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0344\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0338\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0319\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0313\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0325\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0286\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0305\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0311\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0325\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0318\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0299\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0275\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0291\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0269\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0295\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0279\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0277\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0281\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0264\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0263\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0276\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0264\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0260\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0240\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0269\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0274\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0246\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0251\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0293\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0253\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0245\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0243\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0259\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0259\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0255\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0245\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0252\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0232\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0256\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0249\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0255\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0250\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0225\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0226\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0232\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0250\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0230\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0257\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0225\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0248\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0243\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0220\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0242\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0228\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0227\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0240\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0228\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0231\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0216\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0226\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0246\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0230\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0231\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0213\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0220\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0213\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0233\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0224\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0230\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0232\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0228\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0242\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0221\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0243\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0204\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0212\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0209\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0216\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0217\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0218\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0236\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0219\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0231\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0215\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0215\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0217\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0222\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0210\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0217\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0210\n",
      "Model 94 of model WindowRegression with avg smape 15.67: \n",
      "Model Number: 95 of 99 with model WindowRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 95 of model WindowRegression with avg smape 6.64: \n",
      "Model Number: 96 of 99 with model WindowRegression for Validation 1\n",
      "Model 96 of model WindowRegression with avg smape 8.65: \n",
      "Model Number: 97 of 99 with model WindowRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:1135: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 97 of model WindowRegression with avg smape 7.92: \n",
      "Model Number: 98 of 99 with model WindowRegression for Validation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 98 of model WindowRegression with avg smape 22.68: \n",
      "Model Number: 99 of 99 with model WindowRegression for Validation 1\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Model 99 of model WindowRegression with avg smape 16.39: \n",
      "Validation Round: 2\n",
      "Model Number: 1 of 99 with model NVAR for Validation 2\n",
      "Model 1 of model NVAR with avg smape 4.44: \n",
      "Model Number: 2 of 99 with model NVAR for Validation 2\n",
      "Model 2 of model NVAR with avg smape 4.44: \n",
      "Model Number: 3 of 99 with model Ensemble for Validation 2\n",
      "Model 3 of model Ensemble with avg smape 4.43: \n",
      "Model Number: 4 of 99 with model NVAR for Validation 2\n",
      "Model 4 of model NVAR with avg smape 4.42: \n",
      "Model Number: 5 of 99 with model NVAR for Validation 2\n",
      "Model 5 of model NVAR with avg smape 4.42: \n",
      "Model Number: 6 of 99 with model Ensemble for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6 of model Ensemble with avg smape 9.12: \n",
      "Model Number: 7 of 99 with model NVAR for Validation 2\n",
      "Model 7 of model NVAR with avg smape 5.71: \n",
      "Model Number: 8 of 99 with model RollingRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 8 of model RollingRegression with avg smape 11.0: \n",
      "Model Number: 9 of 99 with model SeasonalNaive for Validation 2\n",
      "Model 9 of model SeasonalNaive with avg smape 11.87: \n",
      "Model Number: 10 of 99 with model SeasonalNaive for Validation 2\n",
      "Model 10 of model SeasonalNaive with avg smape 10.44: \n",
      "Model Number: 11 of 99 with model RollingRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 11 of model RollingRegression with avg smape 8.77: \n",
      "Model Number: 12 of 99 with model AverageValueNaive for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 12 of model AverageValueNaive with avg smape 13.62: \n",
      "Model Number: 13 of 99 with model SeasonalNaive for Validation 2\n",
      "Model 13 of model SeasonalNaive with avg smape 10.02: \n",
      "Model Number: 14 of 99 with model Ensemble for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\preprocessing\\_data.py:3237: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 14 of model Ensemble with avg smape 0.0: \n",
      "Model Number: 15 of 99 with model NVAR for Validation 2\n",
      "Model 15 of model NVAR with avg smape 16.58: \n",
      "Model Number: 16 of 99 with model SeasonalNaive for Validation 2\n",
      "Model 16 of model SeasonalNaive with avg smape 7.17: \n",
      "Model Number: 17 of 99 with model NVAR for Validation 2\n",
      "Model 17 of model NVAR with avg smape 8.64: \n",
      "Model Number: 18 of 99 with model GLS for Validation 2\n",
      "Model 18 of model GLS with avg smape 3.21: \n",
      "Model Number: 19 of 99 with model AverageValueNaive for Validation 2\n",
      "Model 19 of model AverageValueNaive with avg smape 7.56: \n",
      "Model Number: 20 of 99 with model ETS for Validation 2\n",
      "Model 20 of model ETS with avg smape 7.99: \n",
      "Model Number: 21 of 99 with model GLS for Validation 2\n",
      "Model 21 of model GLS with avg smape 13.16: \n",
      "Model Number: 22 of 99 with model GLS for Validation 2\n",
      "Model 22 of model GLS with avg smape 13.16: \n",
      "Model Number: 23 of 99 with model SeasonalNaive for Validation 2\n",
      "Model 23 of model SeasonalNaive with avg smape 6.1: \n",
      "Model Number: 24 of 99 with model SeasonalNaive for Validation 2\n",
      "Model 24 of model SeasonalNaive with avg smape 10.03: \n",
      "Model Number: 25 of 99 with model GLM for Validation 2\n",
      "Model 25 of model GLM with avg smape 24.72: \n",
      "Model Number: 26 of 99 with model GLM for Validation 2\n",
      "Model 26 of model GLM with avg smape 16.49: \n",
      "Model Number: 27 of 99 with model GLM for Validation 2\n",
      "Model 27 of model GLM with avg smape 16.49: \n",
      "Model Number: 28 of 99 with model GLM for Validation 2\n",
      "Model 28 of model GLM with avg smape 16.64: \n",
      "Model Number: 29 of 99 with model GLM for Validation 2\n",
      "Model 29 of model GLM with avg smape 16.69: \n",
      "Model Number: 30 of 99 with model GLM for Validation 2\n",
      "Model 30 of model GLM with avg smape 16.79: \n",
      "Model Number: 31 of 99 with model GLM for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:278: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n",
      "  DomainWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 31 of model GLM with avg smape 16.69: \n",
      "Model Number: 32 of 99 with model GLM for Validation 2\n",
      "Model 32 of model GLM with avg smape 16.69: \n",
      "Model Number: 33 of 99 with model LastValueNaive for Validation 2\n",
      "Model 33 of model LastValueNaive with avg smape 8.29: \n",
      "Model Number: 34 of 99 with model GLS for Validation 2\n",
      "Model 34 of model GLS with avg smape 12.96: \n",
      "Model Number: 35 of 99 with model AverageValueNaive for Validation 2\n",
      "Model 35 of model AverageValueNaive with avg smape 18.51: \n",
      "Model Number: 36 of 99 with model LastValueNaive for Validation 2\n",
      "Model 36 of model LastValueNaive with avg smape 8.29: \n",
      "Model Number: 37 of 99 with model LastValueNaive for Validation 2\n",
      "Model 37 of model LastValueNaive with avg smape 8.23: \n",
      "Model Number: 38 of 99 with model LastValueNaive for Validation 2\n",
      "Model 38 of model LastValueNaive with avg smape 9.05: \n",
      "Model Number: 39 of 99 with model LastValueNaive for Validation 2\n",
      "Model 39 of model LastValueNaive with avg smape 9.05: \n",
      "Model Number: 40 of 99 with model LastValueNaive for Validation 2\n",
      "Model 40 of model LastValueNaive with avg smape 8.29: \n",
      "Model Number: 41 of 99 with model ZeroesNaive for Validation 2\n",
      "Model 41 of model ZeroesNaive with avg smape 18.23: \n",
      "Model Number: 42 of 99 with model SeasonalNaive for Validation 2\n",
      "Model 42 of model SeasonalNaive with avg smape 3.92: \n",
      "Model Number: 43 of 99 with model NVAR for Validation 2\n",
      "Model 43 of model NVAR with avg smape 15.68: \n",
      "Model Number: 44 of 99 with model SeasonalNaive for Validation 2\n",
      "Model 44 of model SeasonalNaive with avg smape 16.96: \n",
      "Model Number: 45 of 99 with model UnobservedComponents for Validation 2\n",
      "Model 45 of model UnobservedComponents with avg smape 7.66: \n",
      "Model Number: 46 of 99 with model UnobservedComponents for Validation 2\n",
      "Model 46 of model UnobservedComponents with avg smape 7.66: \n",
      "Model Number: 47 of 99 with model UnobservedComponents for Validation 2\n",
      "Model 47 of model UnobservedComponents with avg smape 7.66: \n",
      "Model Number: 48 of 99 with model UnobservedComponents for Validation 2\n",
      "Model 48 of model UnobservedComponents with avg smape 7.66: \n",
      "Model Number: 49 of 99 with model GLS for Validation 2\n",
      "Model 49 of model GLS with avg smape 11.72: \n",
      "Model Number: 50 of 99 with model GLS for Validation 2\n",
      "Model 50 of model GLS with avg smape 11.72: \n",
      "Model Number: 51 of 99 with model AverageValueNaive for Validation 2\n",
      "Model 51 of model AverageValueNaive with avg smape 4.91: \n",
      "Model Number: 52 of 99 with model LastValueNaive for Validation 2\n",
      "Model 52 of model LastValueNaive with avg smape 9.63: \n",
      "Model Number: 53 of 99 with model RollingRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 53 of model RollingRegression with avg smape 10.05: \n",
      "Model Number: 54 of 99 with model UnobservedComponents for Validation 2\n",
      "Model 54 of model UnobservedComponents with avg smape 7.94: \n",
      "Model Number: 55 of 99 with model RollingRegression for Validation 2\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 8s 7ms/step - loss: 0.2423\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.2387\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.2305\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2344\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.2373\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1963\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.1949\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.1615\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.1245\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.1084\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1058\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.1079\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.1185\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.1125\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1024\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0963\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.1013\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0953\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.1041\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.1008\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0901\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0815\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.1027\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0946\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.1029\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0886\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0963\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0964\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0883\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0945\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0862\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0905\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.1003\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0928\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0804\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0780\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0929\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0824\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0845\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0893\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0771\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0843\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0835\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0946\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0954\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0888\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0878\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0922\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0857\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0860\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0771\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0840\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0793\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0808\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0761\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0777\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0810\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0812\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0737\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0843\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0806\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0874\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0786\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0811\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0896\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0825\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0890\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0781\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0761\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0811\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0832\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0877\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0893\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0723\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0746\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0854\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0815\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0842\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0823\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0823\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0823\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0708\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0806\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0740\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0794\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0862\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0773\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0772\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0788\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0785\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0840\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0765\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0830\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0797\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0716\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0755\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0873\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0763\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0769\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.0827\n",
      "Model 55 of model RollingRegression with avg smape 10.66: \n",
      "Model Number: 56 of 99 with model UnobservedComponents for Validation 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 56 of model UnobservedComponents with avg smape 7.94: \n",
      "Model Number: 57 of 99 with model UnobservedComponents for Validation 2\n",
      "Model 57 of model UnobservedComponents with avg smape 7.8: \n",
      "Model Number: 58 of 99 with model DatepartRegression for Validation 2\n",
      "Model 58 of model DatepartRegression with avg smape 14.96: \n",
      "Model Number: 59 of 99 with model ETS for Validation 2\n",
      "Model 59 of model ETS with avg smape 7.99: \n",
      "Model Number: 60 of 99 with model DatepartRegression for Validation 2\n",
      "Model 60 of model DatepartRegression with avg smape 8.6: \n",
      "Model Number: 61 of 99 with model DatepartRegression for Validation 2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 61 of model DatepartRegression with avg smape 17.87: \n",
      "Model Number: 62 of 99 with model RollingRegression for Validation 2\n",
      "Model 62 of model RollingRegression with avg smape 8.32: \n",
      "Model Number: 63 of 99 with model GLS for Validation 2\n",
      "Model 63 of model GLS with avg smape 4.68: \n",
      "Model Number: 64 of 99 with model GLS for Validation 2\n",
      "Model 64 of model GLS with avg smape 4.68: \n",
      "Model Number: 65 of 99 with model ZeroesNaive for Validation 2\n",
      "Model 65 of model ZeroesNaive with avg smape 8.74: \n",
      "Model Number: 66 of 99 with model ZeroesNaive for Validation 2\n",
      "Model 66 of model ZeroesNaive with avg smape 8.74: \n",
      "Model Number: 67 of 99 with model AverageValueNaive for Validation 2\n",
      "Model 67 of model AverageValueNaive with avg smape 7.67: \n",
      "Model Number: 68 of 99 with model ZeroesNaive for Validation 2\n",
      "Model 68 of model ZeroesNaive with avg smape 9.91: \n",
      "Model Number: 69 of 99 with model RollingRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 69 of model RollingRegression with avg smape 6.42: \n",
      "Model Number: 70 of 99 with model ETS for Validation 2\n",
      "Model 70 of model ETS with avg smape 18.23: \n",
      "Model Number: 71 of 99 with model ETS for Validation 2\n",
      "Model 71 of model ETS with avg smape 18.44: \n",
      "Model Number: 72 of 99 with model LastValueNaive for Validation 2\n",
      "Model 72 of model LastValueNaive with avg smape 8.35: \n",
      "Model Number: 73 of 99 with model ETS for Validation 2\n",
      "Model 73 of model ETS with avg smape 10.44: \n",
      "Model Number: 74 of 99 with model RollingRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 74 of model RollingRegression with avg smape 7.72: \n",
      "Model Number: 75 of 99 with model UnobservedComponents for Validation 2\n",
      "Model 75 of model UnobservedComponents with avg smape 16.05: \n",
      "Model Number: 76 of 99 with model DatepartRegression for Validation 2\n",
      "Model 76 of model DatepartRegression with avg smape 8.38: \n",
      "Model Number: 77 of 99 with model DatepartRegression for Validation 2\n",
      "Model 77 of model DatepartRegression with avg smape 16.24: \n",
      "Model Number: 78 of 99 with model DatepartRegression for Validation 2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Model 78 of model DatepartRegression with avg smape 23.01: \n",
      "Model Number: 79 of 99 with model RollingRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 79 of model RollingRegression with avg smape 12.13: \n",
      "Model Number: 80 of 99 with model DatepartRegression for Validation 2\n",
      "Model 80 of model DatepartRegression with avg smape 7.68: \n",
      "Model Number: 81 of 99 with model ETS for Validation 2\n",
      "Model 81 of model ETS with avg smape 7.98: \n",
      "Model Number: 82 of 99 with model AverageValueNaive for Validation 2\n",
      "Model 82 of model AverageValueNaive with avg smape 11.54: \n",
      "Model Number: 83 of 99 with model DatepartRegression for Validation 2\n",
      "Model 83 of model DatepartRegression with avg smape 11.06: \n",
      "Model Number: 84 of 99 with model ZeroesNaive for Validation 2\n",
      "Model 84 of model ZeroesNaive with avg smape 10.61: \n",
      "Model Number: 85 of 99 with model ZeroesNaive for Validation 2\n",
      "Model 85 of model ZeroesNaive with avg smape 10.61: \n",
      "Model Number: 86 of 99 with model AverageValueNaive for Validation 2\n",
      "Model 86 of model AverageValueNaive with avg smape 4.8: \n",
      "Model Number: 87 of 99 with model ZeroesNaive for Validation 2\n",
      "Model 87 of model ZeroesNaive with avg smape 10.58: \n",
      "Model Number: 88 of 99 with model ZeroesNaive for Validation 2\n",
      "Model 88 of model ZeroesNaive with avg smape 10.58: \n",
      "Model Number: 89 of 99 with model ETS for Validation 2\n",
      "Model 89 of model ETS with avg smape 7.94: \n",
      "Model Number: 90 of 99 with model AverageValueNaive for Validation 2\n",
      "Model 90 of model AverageValueNaive with avg smape 8.31: \n",
      "Model Number: 91 of 99 with model ETS for Validation 2\n",
      "Model 91 of model ETS with avg smape 8.74: \n",
      "Model Number: 92 of 99 with model WindowRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 92 of model WindowRegression with avg smape 9.63: \n",
      "Model Number: 93 of 99 with model WindowRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 93 of model WindowRegression with avg smape 6.26: \n",
      "Model Number: 94 of 99 with model WindowRegression for Validation 2\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 8s 26ms/step - loss: 0.0662\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0681\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0608\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0495\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0509\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0423\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0421\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0372\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0371\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0376\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0363\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0328\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0348\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0329\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0327\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0311\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0326\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0343\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0328\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0309\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0293\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0303\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0314\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0292\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0292\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0323\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.028 - 0s 9ms/step - loss: 0.0286\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0298\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0318\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0289\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0243\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0293\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0264\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0292\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0275\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0281\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0300\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0263\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0270\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0278\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0297\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0265\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0264\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0280\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0268\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0242\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0265\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0249\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0268\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0248\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0261\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0249\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0257\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0264\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0230\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0269\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0244\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0258\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0253\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0249\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0269\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0257\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0275\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0246\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0242\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0238\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0231\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0250\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0244\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0252\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0245\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0260\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0251\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0220\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0231\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0242\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.025 - 0s 7ms/step - loss: 0.0240\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0247\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0239\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0242\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0231\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0246\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0225\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0220\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0241\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0250\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0235\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0234\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0228\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0236\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0228\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0240\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0234\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0213\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0225\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0224\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0248\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0219\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0217\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0221\n",
      "Model 94 of model WindowRegression with avg smape 4.8: \n",
      "Model Number: 95 of 99 with model WindowRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 95 of model WindowRegression with avg smape 13.28: \n",
      "Model Number: 96 of 99 with model WindowRegression for Validation 2\n",
      "Model 96 of model WindowRegression with avg smape 7.03: \n",
      "Model Number: 97 of 99 with model WindowRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\autots\\models\\sklearn.py:1135: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.regr = self.regr.fit(X, Y)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 97 of model WindowRegression with avg smape 12.54: \n",
      "Model Number: 98 of 99 with model WindowRegression for Validation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 98 of model WindowRegression with avg smape 33.33: \n",
      "Model Number: 99 of 99 with model WindowRegression for Validation 2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 99 of model WindowRegression with avg smape 17.62: \n"
     ]
    }
   ],
   "source": [
    "from autots import AutoTS\n",
    "model = AutoTS(forecast_length=10, frequency='infer', ensemble='simple', drop_data_older_than_periods=200)\n",
    "model = model.fit(data, date_col='Date', value_col='Close', id_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas.Ali\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\preprocessing\\_data.py:3237: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DogeCoin Price Prediction\n",
      "            Close\n",
      "2021-10-16    NaN\n",
      "2021-10-17    NaN\n",
      "2021-10-18    NaN\n",
      "2021-10-19    NaN\n",
      "2021-10-20    NaN\n",
      "2021-10-21    NaN\n",
      "2021-10-22    NaN\n",
      "2021-10-23    NaN\n",
      "2021-10-24    NaN\n",
      "2021-10-25    NaN\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict()\n",
    "forecast = prediction.forecast\n",
    "print(\"DogeCoin Price Prediction\")\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many machine learning approaches that we can use for predicting the future prices of Dogecoin. In this file, we introduced how we can predict the future prices of Dogecoin by using the autots library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
